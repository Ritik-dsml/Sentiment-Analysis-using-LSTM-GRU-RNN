# ğŸ” Sentiment Analysis System

**(RNN / LSTM / GRU | Streamlit | TensorFlow)**

A **production-ready Deep Learning Sentiment Analysis application** that classifies text into
**Positive, Neutral, or Negative** sentiments using **Recurrent Neural Networks** and a **Streamlit web interface**.

> âœ… Inference pipeline is **fully aligned with training & CLI logic**
> âœ… Avoids prediction drift (common real-world ML issue)
> âœ… Designed for **portfolio, interviews, and deployment**

---

## ğŸš€ Live Capabilities

* ğŸ§  Deep Learningâ€“based sentiment classification
* ğŸ“ Real-time text input
* ğŸ¨ Clean, intuitive Streamlit UI
* ğŸ”„ Consistent predictions across:

  * Training
  * CLI
  * Web frontend
* ğŸ“¦ Deployment-ready structure

---

## ğŸ§  Models Used

| Model     | Purpose                       |
| --------- | ----------------------------- |
| SimpleRNN | Baseline sequential learning  |
| LSTM      | Long-term dependency capture  |
| GRU       | Efficient alternative to LSTM |

All models are trained for **3-class classification**:

```
Negative | Neutral | Positive
```

---

## ğŸ“Š Dataset

* **Source:** Twitter Sentiment Analysis Dataset (Kaggle)
* **Samples:** ~31,000 tweets
* **Classes:** Negative, Neutral, Positive
* **Encoding:** LabelEncoder + One-Hot Encoding

---

## ğŸ§¹ Text Preprocessing (Critical)

Identical preprocessing is applied during:

* Training
* CLI prediction
* Streamlit inference

```python
text = text.lower()
text = re.sub(r'[^a-zA-Z\s]', '', text)
```

âš ï¸ **Any mismatch here causes incorrect predictions**
(This project explicitly solves that issue.)

---

## ğŸ—ï¸ System Architecture

### ğŸ”¹ High-Level Architecture

```mermaid
flowchart LR
    A[User Input Text] --> B[Text Cleaning]
    B --> C[Tokenizer]
    C --> D[Padding & Sequencing]
    D --> E[Deep Learning Model]
    E --> F[Softmax Output]
    F --> G[Sentiment Label]
```

---

### ğŸ”¹ Training Pipeline

```mermaid
flowchart LR
    A[Raw Tweets] --> B[Text Cleaning]
    B --> C[Tokenizer Fit]
    C --> D[Padding]
    D --> E[Train RNN / LSTM / GRU]
    E --> F[Softmax Output Layer]
    F --> G[Model Saved \( .h5 \)]
    C --> H[Tokenizer Saved \( .pkl \)]
```
---

### ğŸ”¹ Inference Pipeline (CLI & Streamlit)

```mermaid
flowchart LR
    A[Input Text] --> B[Clean Text]
    B --> C[Load Tokenizer]
    C --> D[Pad Sequences]
    D --> E[Load Trained Model]
    E --> F[Softmax Prediction]
    F --> G[Argmax]
    G --> H[Sentiment Output]
```

> ğŸ§  **Key Engineering Insight:**
> Training, CLI, and Streamlit use the **exact same pipeline**, preventing prediction drift.

---

## ğŸ–¥ï¸ Streamlit Frontend

**Features:**

* Real-time prediction
* Emoji + color-coded sentiment
* Minimal, recruiter-friendly UI
* No misleading confidence display
* Stable and deterministic predictions

---

## ğŸ§ª Example Predictions

| Input Text                      | Output      |
| ------------------------------- | ----------- |
| that boy is having nice haircut | Positive ğŸ˜Š |
| he is a bad boy                 | Negative ğŸ˜ |
| They are Boys                   | Neutral ğŸ˜  |

---

## ğŸ“‚ Project Structure

```
sentiment-analysis-project/
â”‚
â”œâ”€â”€ app.py              # Streamlit frontend
â”œâ”€â”€ rnn_model.h5        # Trained RNN model
â”œâ”€â”€ tokenizer.pkl       # Saved tokenizer
â”œâ”€â”€ requirements.txt    # Dependencies
â”œâ”€â”€ README.md           # Documentation
```

---

## âš™ï¸ Installation & Usage

### 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/Ritik-dsml/sentiment-analysis-project.git
cd sentiment-analysis-project
```

### 2ï¸âƒ£ Create Virtual Environment (Recommended)

```bash
python -m venv venv
venv\Scripts\activate
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Run Application

```bash
streamlit run app.py
```

---

## ğŸ“¦ Requirements

```
streamlit
tensorflow
keras
numpy
pandas
scikit-learn
matplotlib
seaborn
```

---

## ğŸ§  Key ML Engineering Learnings

* Importance of **identical preprocessing** at training & inference
* Padding (`pre`) and sequence length consistency
* Label decoding alignment with `LabelEncoder`
* Avoiding silent ML bugs during deployment

> ğŸ’¡ This mirrors real-world production ML issues faced in industry.

---

## ğŸ‘¨â€ğŸ’» Author

**Ritik Kumar**
Data Analyst | AI & ML Enthusiast

* ğŸ”— LinkedIn: [https://www.linkedin.com/in/ritik-kumar-mlai](https://www.linkedin.com/in/ritik-kumar-mlai)
* ğŸ’» GitHub: [https://github.com/Ritik-dsml](https://github.com/Ritik-dsml)

---

## ğŸš€ Future Enhancements

* Confidence score visualization
* CSV bulk sentiment analysis
* Explainable AI (word importance)
* Transformer upgrade (BERT / DistilBERT)
* Cloud deployment (Streamlit Cloud / AWS)

---

## â­ Why This Project Stands Out

âœ” End-to-end ML system
âœ” Real deployment debugging experience
âœ” Clean architecture
âœ” Interview-ready explanations
âœ” Recruiter-friendly presentation

---

